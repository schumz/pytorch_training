{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Générer le transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[232]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BILINEAR\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision \n",
        "import torch\n",
        "\n",
        "weights = torchvision.models.ResNet101_Weights.DEFAULT\n",
        "transformer = weights.transforms()\n",
        "transformer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Générer les dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from make_dataloader import make_dataloader\n",
        "from pathlib import Path\n",
        "\n",
        "image_path=Path(\"data/pizza_steak_sushi/\")\n",
        "\n",
        "test_path=image_path/\"test\"\n",
        "train_path=image_path/\"train\"\n",
        "\n",
        "\n",
        "train_dl, test_dl, class_names = make_dataloader(train_path=train_path,\n",
        "                test_path=test_path,\n",
        "                transformer=transformer,\n",
        "                batch_size=32)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Générer le modèle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Adam\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Adam\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "C:\\Users\\Adam\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchinfo\\torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
            "C:\\Users\\Adam\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return super().__sizeof__() + self.nbytes()\n"
          ]
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = torchvision.models.resnet101(pretrained=True).to(device)\n",
        "\n",
        "summary(model=model,\n",
        "        input_size=(1,3,224,224),\n",
        "        col_names=['input_size','output_size','num_params','trainable'],\n",
        "        row_settings=['var_names'])\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.fc=nn.Sequential(\n",
        "    nn.Linear(in_features=2048, out_features=3, bias=True))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Générer l'optimizer, la loss_fn et le writer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from train_modele import train, train_step, test_step\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import mlflow\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "##################################################################\n",
        "optimizer = torch.optim.Adam(params=model.parameters(),lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "##################################################################\n",
        "\n",
        "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "log_dir = os.path.join(\"runs\",date,\"test\")\n",
        "summary=SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "##################################################################\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Entraîner le modèle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [00:19<01:18, 19.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 0.6051 | train_acc: 0.7305 | test_loss: 0.5417 | test_acc: 0.8030\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [00:38<00:57, 19.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 | train_loss: 0.6517 | train_acc: 0.6367 | test_loss: 0.4137 | test_acc: 0.9593\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [00:57<00:38, 19.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 | train_loss: 0.7541 | train_acc: 0.6914 | test_loss: 0.4314 | test_acc: 0.8352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [01:16<00:19, 19.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 | train_loss: 0.4948 | train_acc: 0.8164 | test_loss: 0.5791 | test_acc: 0.7415\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [01:36<00:00, 19.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 | train_loss: 0.4530 | train_acc: 0.7617 | test_loss: 0.3096 | test_acc: 0.9688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "2023/05/20 16:10:44 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.0+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2023/05/20 16:10:48 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.15.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torchvision==0.15.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "ename": "MlflowException",
          "evalue": "The configured tracking uri scheme: 'file' is invalid for use with the proxy mlflow-artifact scheme. The allowed tracking schemes are: {'http', 'https'}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_model\u001b[39m=\u001b[39m train(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      2\u001b[0m                       train_dataloader\u001b[39m=\u001b[39;49mtrain_dl,\n\u001b[0;32m      3\u001b[0m                       test_dataloader\u001b[39m=\u001b[39;49mtest_dl,\n\u001b[0;32m      4\u001b[0m                       device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m      5\u001b[0m                       optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m      6\u001b[0m                       loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m      7\u001b[0m                       epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m                       writer\u001b[39m=\u001b[39;49msummary,\n\u001b[0;32m      9\u001b[0m                       )\n",
            "File \u001b[1;32md:\\Adam\\Pytorch\\exercises\\train_modele.py:112\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, device, optimizer, loss_fn, epochs, writer)\u001b[0m\n\u001b[0;32m    109\u001b[0m log_dir \u001b[39m=\u001b[39m writer\u001b[39m.\u001b[39mlog_dir\n\u001b[0;32m    111\u001b[0m \u001b[39m# Log the entire model\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m mlflow\u001b[39m.\u001b[39;49mpytorch\u001b[39m.\u001b[39;49mlog_model(model, \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m, registered_model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmy_model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    114\u001b[0m \u001b[39m# Log the writer's log directory as an artifact\u001b[39;00m\n\u001b[0;32m    115\u001b[0m mlflow\u001b[39m.\u001b[39mlog_artifact(log_dir, artifact_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard_logs\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\pytorch\\__init__.py:311\u001b[0m, in \u001b[0;36mlog_model\u001b[1;34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, requirements_file, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[39mLog a PyTorch model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39m    PyTorch logged models\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m pickle_module \u001b[39m=\u001b[39m pickle_module \u001b[39mor\u001b[39;00m mlflow_pytorch_pickle_module\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m Model\u001b[39m.\u001b[39mlog(\n\u001b[0;32m    312\u001b[0m     artifact_path\u001b[39m=\u001b[39martifact_path,\n\u001b[0;32m    313\u001b[0m     flavor\u001b[39m=\u001b[39mmlflow\u001b[39m.\u001b[39mpytorch,\n\u001b[0;32m    314\u001b[0m     pytorch_model\u001b[39m=\u001b[39mpytorch_model,\n\u001b[0;32m    315\u001b[0m     conda_env\u001b[39m=\u001b[39mconda_env,\n\u001b[0;32m    316\u001b[0m     code_paths\u001b[39m=\u001b[39mcode_paths,\n\u001b[0;32m    317\u001b[0m     pickle_module\u001b[39m=\u001b[39mpickle_module,\n\u001b[0;32m    318\u001b[0m     registered_model_name\u001b[39m=\u001b[39mregistered_model_name,\n\u001b[0;32m    319\u001b[0m     signature\u001b[39m=\u001b[39msignature,\n\u001b[0;32m    320\u001b[0m     input_example\u001b[39m=\u001b[39minput_example,\n\u001b[0;32m    321\u001b[0m     await_registration_for\u001b[39m=\u001b[39mawait_registration_for,\n\u001b[0;32m    322\u001b[0m     requirements_file\u001b[39m=\u001b[39mrequirements_file,\n\u001b[0;32m    323\u001b[0m     extra_files\u001b[39m=\u001b[39mextra_files,\n\u001b[0;32m    324\u001b[0m     pip_requirements\u001b[39m=\u001b[39mpip_requirements,\n\u001b[0;32m    325\u001b[0m     extra_pip_requirements\u001b[39m=\u001b[39mextra_pip_requirements,\n\u001b[0;32m    326\u001b[0m     metadata\u001b[39m=\u001b[39mmetadata,\n\u001b[0;32m    327\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    328\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\models\\model.py:554\u001b[0m, in \u001b[0;36mModel.log\u001b[1;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m mlflow_model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(artifact_path\u001b[39m=\u001b[39martifact_path, run_id\u001b[39m=\u001b[39mrun_id, metadata\u001b[39m=\u001b[39mmetadata)\n\u001b[0;32m    553\u001b[0m flavor\u001b[39m.\u001b[39msave_model(path\u001b[39m=\u001b[39mlocal_path, mlflow_model\u001b[39m=\u001b[39mmlflow_model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 554\u001b[0m mlflow\u001b[39m.\u001b[39;49mtracking\u001b[39m.\u001b[39;49mfluent\u001b[39m.\u001b[39;49mlog_artifacts(local_path, mlflow_model\u001b[39m.\u001b[39;49martifact_path)\n\u001b[0;32m    555\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    556\u001b[0m     mlflow\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mfluent\u001b[39m.\u001b[39m_record_logged_model(mlflow_model)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\tracking\\fluent.py:818\u001b[0m, in \u001b[0;36mlog_artifacts\u001b[1;34m(local_dir, artifact_path)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39mLog all the contents of a local directory as artifacts of the run. If no run is active,\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[39mthis method will create a new active run.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39m        mlflow.log_artifacts(\"data\", artifact_path=\"states\")\u001b[39;00m\n\u001b[0;32m    816\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    817\u001b[0m run_id \u001b[39m=\u001b[39m _get_or_start_run()\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[1;32m--> 818\u001b[0m MlflowClient()\u001b[39m.\u001b[39;49mlog_artifacts(run_id, local_dir, artifact_path)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\tracking\\client.py:1074\u001b[0m, in \u001b[0;36mMlflowClient.log_artifacts\u001b[1;34m(self, run_id, local_dir, artifact_path)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_artifacts\u001b[39m(\n\u001b[0;32m   1031\u001b[0m     \u001b[39mself\u001b[39m, run_id: \u001b[39mstr\u001b[39m, local_dir: \u001b[39mstr\u001b[39m, artifact_path: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[39m    Write a directory of files to the remote ``artifact_uri``.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[39m        is_dir: True\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1074\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mlog_artifacts(run_id, local_dir, artifact_path)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:448\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_artifacts\u001b[1;34m(self, run_id, local_dir, artifact_path)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_artifacts\u001b[39m(\u001b[39mself\u001b[39m, run_id, local_dir, artifact_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    442\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39m    Write a directory of files to the remote ``artifact_uri``.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[39m    :param local_dir: Path to the directory of files to write.\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[39m    :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 448\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_artifact_repo(run_id)\u001b[39m.\u001b[39mlog_artifacts(local_dir, artifact_path)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:416\u001b[0m, in \u001b[0;36mTrackingServiceClient._get_artifact_repo\u001b[1;34m(self, run_id)\u001b[0m\n\u001b[0;32m    412\u001b[0m run \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_run(run_id)\n\u001b[0;32m    413\u001b[0m artifact_uri \u001b[39m=\u001b[39m add_databricks_profile_info_to_artifact_uri(\n\u001b[0;32m    414\u001b[0m     run\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39martifact_uri, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracking_uri\n\u001b[0;32m    415\u001b[0m )\n\u001b[1;32m--> 416\u001b[0m artifact_repo \u001b[39m=\u001b[39m get_artifact_repository(artifact_uri)\n\u001b[0;32m    417\u001b[0m \u001b[39m# Cache the artifact repo to avoid a future network call, removing the oldest\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[39m# entry in the cache if there are too many elements\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(TrackingServiceClient\u001b[39m.\u001b[39m_artifact_repos_cache) \u001b[39m>\u001b[39m \u001b[39m1024\u001b[39m:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\store\\artifact\\artifact_repository_registry.py:106\u001b[0m, in \u001b[0;36mget_artifact_repository\u001b[1;34m(artifact_uri)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_artifact_repository\u001b[39m(artifact_uri):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get an artifact repository from the registry based on the scheme of artifact_uri\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[39m    :param artifact_uri: The artifact store URI. This URI is used to select which artifact\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m             requirements.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m _artifact_repository_registry\u001b[39m.\u001b[39;49mget_artifact_repository(artifact_uri)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\store\\artifact\\artifact_repository_registry.py:72\u001b[0m, in \u001b[0;36mArtifactRepositoryRegistry.get_artifact_repository\u001b[1;34m(self, artifact_uri)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m repository \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[0;32m     67\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not find a registered artifact repository for: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCurrently registered schemes are: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     69\u001b[0m             artifact_uri, \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_registry\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m     70\u001b[0m         )\n\u001b[0;32m     71\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m repository(artifact_uri)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\store\\artifact\\mlflow_artifacts_repo.py:45\u001b[0m, in \u001b[0;36mMlflowArtifactsRepository.__init__\u001b[1;34m(self, artifact_uri)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, artifact_uri):\n\u001b[1;32m---> 45\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresolve_uri(artifact_uri, get_tracking_uri()))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\store\\artifact\\mlflow_artifacts_repo.py:59\u001b[0m, in \u001b[0;36mMlflowArtifactsRepository.resolve_uri\u001b[1;34m(cls, artifact_uri, tracking_uri)\u001b[0m\n\u001b[0;32m     56\u001b[0m _validate_port_mapped_to_hostname(uri_parse)\n\u001b[0;32m     58\u001b[0m \u001b[39m# Check that tracking uri is http or https\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m _validate_uri_scheme(track_parse\u001b[39m.\u001b[39;49mscheme)\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m uri_parse\u001b[39m.\u001b[39mpath \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# root directory; build simple path\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     resolved \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mbase_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00muri_parse\u001b[39m.\u001b[39mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\mlflow\\store\\artifact\\mlflow_artifacts_repo.py:35\u001b[0m, in \u001b[0;36m_validate_uri_scheme\u001b[1;34m(scheme)\u001b[0m\n\u001b[0;32m     33\u001b[0m allowable_schemes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mhttp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhttps\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m scheme \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowable_schemes:\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[0;32m     36\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe configured tracking uri scheme: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mscheme\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is invalid for use with the proxy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmlflow-artifact scheme. The allowed tracking schemes are: \u001b[39m\u001b[39m{\u001b[39;00mallowable_schemes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m     )\n",
            "\u001b[1;31mMlflowException\u001b[0m: The configured tracking uri scheme: 'file' is invalid for use with the proxy mlflow-artifact scheme. The allowed tracking schemes are: {'http', 'https'}"
          ]
        }
      ],
      "source": [
        "training_model= train(model=model,\n",
        "                      train_dataloader=train_dl,\n",
        "                      test_dataloader=test_dl,\n",
        "                      device=device,\n",
        "                      optimizer=optimizer,\n",
        "                      loss_fn=loss_fn,\n",
        "                      epochs=5,\n",
        "                      writer=summary,\n",
        "                      )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyP4+Nwb43yrG43qNz11d5C4",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "07_pytorch_experiment_tracking_exercise_template.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
